**Conditional independence** is a fundamental concept in probability and Bayesian networks. It describes a situation where **two variables are independent when given knowledge of a third variable**.

---

### üîç **Definition**

Random variables **A** and **B** are **conditionally independent** given a third variable **C** if:

$$
P(A, B \mid C) = P(A \mid C) \cdot P(B \mid C)
$$

This means:

> Once we know the value of **C**, knowing **B** gives us no extra information about **A**, and vice versa.

We write this as:

$$
A \perp B \mid C
$$

---

### üß† **Example: The Classic "Sprinkler" Network**

```
Rain ‚Üí WetGrass ‚Üê Sprinkler
```

Variables:

* **Rain** (R)
* **Sprinkler** (S)
* **WetGrass** (W)

In this network:

* Rain and Sprinkler both influence whether the grass is wet.
* **Without knowing WetGrass**, Rain and Sprinkler are **independent**.
* But **given that WetGrass is wet**, Rain and Sprinkler become **dependent** ‚Äî if we know it rained, the chance that the sprinkler was on decreases (because we now have an explanation for why the grass is wet).

However, in another setup:

```
Rain ‚Üí Clouds ‚Üí WetGrass
```

Here:

* **Rain and WetGrass** are **conditionally independent given Clouds**.
* If we know the cloud status, knowing Rain doesn‚Äôt give extra information about WetGrass.

---

### ‚úÖ **Why It Matters**

In **Bayesian networks**, conditional independencies:

* Allow **efficient computation** (we don‚Äôt need the full joint distribution).
* Reflect **causal or statistical separations** between variables.
* Are encoded by the **graph structure**: you can often "read off" conditional independencies using a method called **d-separation**.

---

Would you like a hands-on example with numbers or a visual graph to explore this further?
	